% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cluster.R
\name{clusterSetup}
\alias{clusterSetup}
\title{High level wrapper around parallelly::makeClusterPSOCK for many multi-core machines}
\usage{
clusterSetup(
  workers,
  objsToExport,
  reqdPkgs,
  quotedExtra,
  libPaths = .libPaths()[1],
  doSpeedTest = FALSE,
  envir = parent.frame(),
  numCoresNeeded = ceiling(detectCores() * 0.8),
  adjustments = rep(1, length(workers))
)
}
\arguments{
\item{workers}{A character vector of ips (including possily 'localhost') of length
one each ip, or a character vector of possibly repeated ips, and possibly including
'localhost', such that the length is equal to \code{workers}}

\item{objsToExport}{A character vector naming the objects that should be copied over
to each worker in the cluster.}

\item{reqdPkgs}{A character vector of packages (using `Require` formatting, so can
specify GitHub packages and minimum version number) that should be (installed if necessary)
and loaded (i.e., in a `require` call).}

\item{quotedExtra}{An optional quoted command to run in single cluster (i.e., once per unique
worker) after `.libPaths(libPath)` is run, e.g., forcing an `install.packages`. See
example.}

\item{libPaths}{The path in which the R packages should be installed and loaded from.}

\item{doSpeedTest}{Logical. If \code{TRUE} (and \code{workers} is longer than 1), then
a small (e.g., 6 second) test will be run on each worker to test the raw cpu speed using
loops and \code{rnorm}.}

\item{envir}{The environment in which to find the \code{objsToExport}, defaults to
\code{parent.frame()}}

\item{numCoresNeeded}{A numeric of length 1, indicating the desired number of workers.}

\item{adjustments}{A vector of length \code{workers} that can be used to artificially
adjust (i.e., weight) the workers more or less than others. These numbers are
relative to each other, so \code{c(1.2, 0.8)} will result in 50% more cores
than estimated during \code{doSpeedTest} on the first worker than the second.
Default is 1.}
}
\description{
This is create to only work with machines that are successfully linked with ssh keys. It
uses reverse tunnelling, so the keys are only needed in one direction (e.g., from the
main to the workers).
One common configuration for High Performance Clusters is several virtual machines (VM),
each with multiple cores. Usually, 1/2 of those cores are hyperthreaded, which means
that when all cores are utilized on a single VM, the per-core speed will be substantially
slower than if only the physical cores are used. Similarly, when R sets up a
cluster, it doesn't know that collections of the multiple cores are actually on
the same virtual machine. When moving large objects "to the cores", it is actually
substantially faster to move the files to one core per machine, write the objects
to disk locally on that virtual machine, then spawn all the desired cores per machine,
reading locally the objects back into memory in each R session core. Similarly,
installing required packages should also be done only once per VM, as each core
shares a common disk.
}
\details{
This function does these steps:

\enumerate{
  \item spawn 1 core per worker;
  \item copy all needed files to that core,
  \item write those objects to disk locally,
  \item install any missing packages or package versions. The default is to
        use the same library path on the remote machines as the local machine. It
        is likely a good idea to make sure the \code{libPath} is not the default
        personal library on any of the other \code{workers};
  \item optionally, test brute cpu speed on each unique worker to determine the
         relative speed of the VMs ;
  \item if \code{workers} is of length 1 per ip, or if \code{workers} is not
           sufficiently long to create the cluster of \code{numCoresNeeded}, then
           it will distribute the \code{numCoresNeeded} across workers, minimizing
           the number of hyperthreaded cores, and using more faster cores than slower
           cores;
  \item shut down the cluster of "single-core-per-VM",
  \item start a new possibly multi-core cluster with all desired cores per VM,
          either given as a precise
          set of \code{workers}, or letting the function determine the set;
  \item read all objects into memory in each R-session (i.e., 1 per core);
  \item return the multi-core cluster object for use by user.
}

1) spawn 1 core per worker, 1a) optionally, test brute cpu speed
on each unique worker to determine the relative speed of the VMs 1b) if \code{workers}
is of length 1 per ip, or if \code{workers} is not sufficiently long to create the
cluster of \code{numCoresNeeded}
}
\note{
If working on Ubuntu machines, the default is to install binary packages from the
Rstudio binary mirror of CRAN. If `source` packages are required (e.g., for compiling
spatial R packages on a system that has recently updated its system spatial libraries,
such as GDAL, GEOS etc.), then this will have to be done manually. See example.
}
\examples{
\dontrun{
largeObj <- rnorm(1e6)
ips <- c("localhost", "10.20.0.213")
objsToExport <- "largeObj"
reqdPkgs <- c("achubaty/amc@development", "raster (>= 3.4-5)")
cl <- clusterSetup(workers = ips, objsToExport = objsToExport,
                   reqdPkgs = reqdPkgs,
                   numCoresNeeded = 4)
# Now use the cl with parallel or DEoptim
parallel::clusterEvalQ(cl, rnorm(1))
parallel::stopCluster(cl)

# Installing some packages from source on linux (source is default)
cl <- clusterSetup(workers = ips, objsToExport = objsToExport,
                   reqdPkgs = reqdPkgs,
                   quotedExtra = quote(install.packages(c("rgdal", "rgeos", "sf",
                                       "sp", "raster", "terra", "lwgeom"),
                                       repos = "https://cran.rstudio.com")),
                   numCoresNeeded = 4)
parallel::stopCluster(cl)

}
}
\author{
Eliot McIntire
}
